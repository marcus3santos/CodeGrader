#+TITLE: CodeGrader
#+Options: tc:t

CodeGrader is an automated grader for Lisp programming practicum
assessments.


* Dependencies

Follow the instructions available [[https://marcus3santos.github.io/lisp-ide.html][here to install the Lisp programming environment]].

  
* Installation

Type the commands below an a linux or Mac terminal:
  #+begin_src shell
   cd ~/quicklisp/local-projects/
   git clone https://github.com/marcus3santos/codegrader.git  
  #+end_src

* The Workflow

- *Step 1: Authoring (The Source)*: 
  The instructor defines the assessment in *SXM (S-expression Markup)*. This file serves as the "single source of truth," encapsulating student-facing questions and public assertions alongside hidden grading tests and reference solutions.
- *Step 2: Compilation (The Artifacts)*: 
  The instructor invokes the ~GEN-EXAM-FILES~ function to compile the SXM source. This automates the production of the assessment package: student-ready documents in *Org-mode, HTML, and PDF*, and a *.data* file containing the metadata and logic required for the autograding engine.
- *Step 3: Practicum (Exam in Progress)*: 
  Students complete the programming practicum using the generated *PDF* as their primary guide. To verify their implementations, students utilize the ~CHK-MY-SOLUTION~ function, which provides immediate feedback by running their code against the public test cases.
- *Step 4: Evaluation (Automated Assessment)*: 
  Once the exam concludes and solutions are collected, the instructor uses CodeGrader to batch-process submissions. The system calculates scores based on two primary metrics:
  1. *Functional Correctness*: Performance against hidden test cases.
  2. *Programming Style*: The structural similarity between the student's implementation and the reference solutions defined in the *.data* file.

The next sections explain each of these workflow steps.

* Phase 1: Authoring Assessments in SXM

Instructors author their programming exams using *SXM (S-expression Markup)*. This structured format leverages Lisp's native syntax to simplify parsing while enabling sophisticated automation and pedagogical tooling.

** SXM Grammar Overview

The following EBNF grammar defines the structure of an SXM document:

#+begin_example
;; --- Top Level Structure ---
<sxm-expression>    ::= <doc-node>
<doc-node>          ::= (:doc (:title <string> :folder <string>) <node>*)

;; --- Shared Node Definition ---
<node>              ::= <string> | <p-node> | <ul-node> | <ol-node> | <li-node> | 
                        <s-node> | <cb-node> | <wa-node> | <q-node>

;; --- Standard Content Nodes ---
<p-node>            ::= (:p <node>*)
<ul-node>           ::= (:ul <node>*)
<ol-node>           ::= (:ol <node>*)
<li-node>           ::= (:li <node>*)
<wa-node>           ::= (:wa <node>*)
<s-node>            ::= (:s ([:level <integer>] [:title <string>]) <node>*)
<cb-node>           ::= (:cb (:language <string>) <node>*)

;; --- Question & Metadata Nodes ---
<q-node>            ::= (:q <q-props> <q-element>*)
<q-props>           ::= ([:title <string>] [:forbidden <list> :penalty <number>])
<q-element>         ::= <node> | <tc-node> | <sols-node>

<tc-node>           ::= (:tc (:function <symbol>) <tc-element>*)
<tc-element>        ::= <string> | <p-node> | <cb-node> | <gvn-node> | <hdn-node>

<gvn-node>          ::= (:gvn <data-element>*)
<hdn-node>          ::= (:hdn <data-element>*)
<data-element>      ::= <string> | <p-node> | <a-node>
<a-node>            ::= (:a <lisp-form> <lisp-form>)

;; --- Solutions ---
<sols-node>         ::= (:sols <sol-node>+)
<sol-node>          ::= (:sol <lisp-code>)
#+end_example

** SXM Implementation Example

The following example demonstrates a question that restricts specific functions and provides both public and hidden test cases.

#+begin_example
(:doc 
  (:title "Common Lisp Practicum Test"
   :folder "~/pt") ;; Target directory for student solutions
  (:q 
    (:penalty 80     ;; Score deduction for using forbidden symbols
     :forbidden (count member)) 
    (:p "This question contains two parts.") 
    (:wa 
      (:p "Write a function COUNT-OCCURRENCES that takes an element and a list and returns the frequency of that element."))
    (:tc (:function count-occurrences)
      (:gvn ;; Public assertions: visible to students via CHK-MY-SOLUTION
        (:a (count-occurrences 3 '(1 2 3 3 3 4)) 3)
        (:a (count-occurrences 'a '(a b a c a)) 3))
      (:hdn ;; Hidden assertions: used by CodeGrader for final evaluation
        (:a (count-occurrences 1 '(1 1 1 1 1)) 5)
        (:a (count-occurrences 'z '(a b c z z)) 2)))
    (:sols
      (:sol
        (defun count-occurrences (a b &optional (acc 0))
          (cond ((null b) acc)
                ((eql a (car b)) (count-occurrences a (cdr b) (1+ acc)))
                (t (count-occurrences a (cdr b) acc)))))
      (:sol
        (defun count-occurrences (a b)
          (count if (lambda (x) (eql x a)) b)))))
#+end_example

** Key Field Definitions

- =:doc= :: The root node. The =:folder= property specifies where the student must save their work for the autograder to locate it.
- =:q= :: Defines a question. Transpiles to a level-1 heading (~*~) in Org-mode.
  - =:penalty= :: Percentage deducted if =:forbidden= symbols are detected in the student's code.
- =:wa= :: (What is Asked) The problem description and student instructions.
- =:tc= :: (Test Cases) Container for functional requirements.
  - =:gvn= :: Publicly accessible test cases.
  - =:hdn= :: Private test cases used for objective evaluation.
- =:a= :: (Assertion) A pair representing a ~(function-call expected-result)~.
- =:sols= :: Contains one or more valid =:sol= (Solution) nodes used for structural similarity grading.

* Phase 2: Generating Assessment Support Files

Once the SXM file is finalized, the instructor compiles the assessment artifacts.

#+begin_src lisp
  SXM> (gen-exam-files "~/tmp/assessment1.sxm" :include-hidden t)
  ;; Output:
  ;; Assessment Org-mode file generated: ~/tmp/Gen-files/assessment1.org
  ;; Assessment HTML file generated:    ~/tmp/Gen-files/assessment1.html
  ;; Assessment PDF file generated:     ~/tmp/Gen-files/assessment1.pdf
  ;; Assessment metadata file generated: ~/tmp/Gen-files/assessment1.data
#+end_src

** Generated Artifacts

The ~GEN-EXAM-FILES~ function creates a =./Gen-files/= directory containing:
1. *Assessment Documentation*: Org-mode, PDF, and HTML versions of the exam.
2. *Grading Metadata*: A =.data= file that stores the assertion, reference solutions, and other relevant data required for automated evaluation.

*NOTE*: Set ~:include-hidden nil~ if you wish to generate a metadata file that excludes private test cases.

** Deployment

To enable automated evaluation, the generated =.data= file must be moved to the CodeGrader central repository:
=~/quicklisp/local-projects/Codegrader/Assessment-data/=

* Phase 3: Student Interaction & The Practicum

During the practicum, students solve the programming problems defined in the assessment. While the **PDF** serves as their primary guide, the students interact with the CodeGrader system through a specialized feedback loop designed to ensure compliance and promote better Lisp programming habits.

** Student Requirements
To succeed in the practicum, students must adhere to two primary constraints:
1. *Function Restrictions*: Students must avoid the ~:forbidden~ functions listed for each question. CodeGrader automatically applies the instructor-defined penalties if these symbols are detected in the submission.
2. *Filesystem Compliance*: Solutions must be saved in the designated folder (e.g., =~/pt1/=) using the exact filenames specified in the assessment header.

** Real-time Feedback with ~CHK-MY-SOLUTION~

To assist students, CodeGrader provides the ~CHK-MY-SOLUTION~ function. This is more than a simple tester; it is a "live" assistant that helps students address issues before the final evaluation.

*** What ~CHK-MY-SOLUTION~ Does:
1. *Path Validation*: Verifies that the file is in the correct directory with the correct name.
2. *Functional Testing*: Executes the solution against the *given* (~:gvn~) test cases.
3. *Style Coaching*: Analyzes the student's code for structural "smells" or poor naming conventions and provides immediate suggestions for improvement.

*** Example Session

If a student is working on Question 1, they can evaluate the following in their REPL:

#+begin_src lisp
CL-USER> (cg:chk-my-solution "~/pt1/q1.lisp")
#+end_src

The output provides a comprehensive report, as shown below:

#+begin_example
[Lisp generated load messages]

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Note: In the messages above, Warning messages starting with
    'WARNING: redefining TEST-RUNTIME::....'
do not affect the results of your test cases.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test Cases Results:
When testing your solution for q1, the results obtained were the following:
- Passed: (LIST-OF-ARRAYS NIL NIL) returned NIL
- Passed: (LIST-OF-ARRAYS '(1 4 2) '(0 5 1)) returned (#(1 0 1) #(4 5 9) #(2 1 3))

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Idiomatic Style & Feedback:
Below is your program and, if applicable, suggestions 
on how to improve your Lisp programming style:

**********************************************************************

(DEFUN LIST-OF-ARRAYS (L1 L2)
  (LABELS ((CREATE-SUM-ARRAY-HELPER (A B)
             (MAKE-ARRAY 3 :INITIAL-CONTENTS (LIST A B (+ A B)))))
    (MAPCAR #'CREATE-SUM-ARRAY-HELPER L1 L2)))
----------------------------------------------------------------------
Helper is not helpful name. A function name should primarily describe
what is returned, and maybe with what input.
----------------------------------------------------------------------
#+end_example



** Benefits of the Feedback Loop
- *Early Correction*: Students catch path or naming errors immediately rather than at the end of the exam.
- *Iterative Improvement*: By viewing the "Style Suggestions," students can refactor their code to achieve a higher "programming style" score during the final evaluation.
- *Confidence*: The functional results provide certainty that the core logic meets the public requirements.

* Grading students' solutions

** Preamble

We make the following assumptions regarding the physical environment where students complete the assessment:
- The IT staff has created a spreadsheet mapping each student ID to a computer ID in the exam room.
- That mapping is shared with the instructor and the students in advance of the assessment.
- In the Linux test environment, the home folder's name is the computer ID.
- In the assessment description, students were asked to store their solutions in a specific subfolder in their home folder, e.g., in the *~/pt/* folder.
- Student files from different assessment sessions are stored by the IT staff in designated folders. For example, under the directory /tmp/cps305PracticumTest/, individual folders named cps305XX (such as cps30501 and cps30502) contain student solutions for specific assessment versions.
- The IT staff used the following command to create a zip archive of these student solutions. This example specifically targets the file structure described above:
  #+begin_src shell
  zip -r cps305PracticumTest.zip /tmp/cps305PracticumTest/cps305*/<computer ID>*/ -x '*/.*' '*/quicklisp/*' '*/Cheatsheet-emacs.pdf' '*/paredit.pdf' '*/test.pdf'
  #+end_src
  

** Steps for grading students' solutions

1. *Create a zipped file containing the students' solutions*: Since
   students from specific sections may have taken different versions
   of the exam, it is crucial to obtain the sections-to-exam-versions
   mapping in advance from the course coordinator to ensure you are running
   CodeGrader  on the solutions written by the students in the correct section, and
   using the correct test cases for the respective exam version.  For
   example, suppose students from sections 03 and 05 are taught by instructor A and took Version 1 of
   the exam, and students from section 10 are taught by instructor B and took Version 2. 

   Assume the parent folder *~/tmp/cps305PracticumTest/* contains the students' solutions from the various sections. Also assume you want to store the zip archive for sections 03 and 05
   in *~/tmp/PT1/Sections/03-05/* (you have already created that folder), the commands below show how to
   create the zip file with the solutions of the students from sectoins 03 and 05:
   #+begin_src shell
     cd ~/tmp/cps305PracticumTest
     (cd cps30503 && zip -r ~/tmp/PT1/Sections/03-05/std-sol.zip *) && (cd cps30505 && zip -r ~/tmp/PT1/Sections/03-05/std-sol.zip *)
   #+end_src
   You would do something similar to zip the solutions for students in section 10.
   #+begin_src shell
     cd ~/tmp/cps305PracticumTest/cps30510
     zip -r ~/tmp/PT1/Sections/10/std-sol.zip *
   #+end_src
   By zipping this way, you would create a zip archive that does not
   include the parent directories (cps30505 and cps30508).

2. *Create a CSV file containing the mapping of students-to-computers*:
   We assume you already have the CSV files containing the
   student-to-computer mapping for each of the course sections. Now,
   based on these CSV files and on the sections-to-exams-versions, you
   should create a CSV file that contains the mappings of all students
   who took a given test version. Each row in that
   spreadsheet should contain the following information: Student ID
   number, Student First Name, Student Last Name, and Room-PC ID
3. *Generate the tooling (.data) file* as explained in section *Generating the Assessment Support Files* above.
4. Create a folder where CodeGrader will store the results. You can give any name to that folder.
5. [ /This is step is not necessary if you are assessing an "ungraded" examination (i.e., an assessment
   whose weight is zero)/ ] On D2L, export the students' *assignment grades* *to a CSV
   file*. Note the following when generating this file
   - Select the following /Export Options/:
     - Key Field:
       - *Both*
     - Grade Values:
       - *Points grade*
     - User Details: 
       - *Last name*
       - *First name*
  - /Choose grades to Export/: Choose only one of the listed grade items. If the grade item contains subitems, choose the appropriate subitem. For example: if a /Practicum Test/ grade item contains subitems representing the versions of the test, choose the subitem representing the Practicum Test version you are interested in grading.
  Below is an example of a CSV file exported by D2L:
  #+begin_example
     OrgDefinedId,Username,Last Name,First Name,Practicum Test 1 - Version 1 Points Grade <Numeric MaxPoints:100 Weight:10 Category:Practicum Test 1 CategoryWeight:10>,End-of-Line Indicator
     #500583619,#TTiger,Tigertongue,Tim,,#
     #500585612,#Patrick97,Pearson,Patrick,,#
     #501585619,#Towhander,Twohands,Tony,,#
     #500586619,#Zain1997,Zodson,Zain,,#
     #500585619,#Coopercat,Cooper,Cain,,#
     #500585119,#Hammermann,Odinson,Thor,,#
  #+end_example
  For more information, visit [[https://www.torontomu.ca/courses/instructors/tutorials/grades/grades-export-import/]]
6. Launch sbcl from the command line
   #+begin_src shell
     rlwrap sbcl --dynamic-space-size 20480
   #+end_src
7. To load the codegrader, type the following commands on the REPL:
   #+begin_src lisp
          (ql:quickload :codegrader)
   #+end_src
8. To run the students' solutions through CodeGrader, type the command
   below on the CodeGrader REPL: (NOTE: once you launch CodeGrader, it
   will start executing the students' solutions; consequently, it will
   display on the REPL buffer all error/warning messages and output
   generated by the student's solution. CodeGrader will be done
   marking when you see the message =Exam grading complete!= displayed
   on the REPL window buffer.)
   #+begin_src lisp
     (cg:grade-exam submissions map tooling-file results-folder exam-grades-export-file)
   #+end_src
   where:
   - ~submissions~ is a string representing the full path and name of
     the zipped file containing the students' solutions, e.g.,
     ~/Users/johndoe/Zipped-solutions/std-sol.zip~
   - ~map~  is a string representing the full path and name of of the csv file storing the student-to-pc mapping.
   - ~tooling-file~ is a string representing the full path for the tooling (.data) file.
   - ~results-folder~ is a string representing the full path for a folder
    where you want codegrader to store the results (the students'
    marks and log files). For example, if you provide the path
    #+begin_example
    "/Users/johndoe/A1/"
    #+end_example
    then CodeGrader will create its
    files/subfolders inside folder ~/Users/johndoe/A1/~.
   - (optional) ~exam-grades-export-file~ is a string representing the full path for
     the D2L exam grades exported by D2L

* Output

CodeGrader generates the following files in the =results= folder (see above):
- A csv spreadsheet file called ~grades.csv~  This is a D2L-importable
  grades file and it is created based on the ~exam-grades-export-file~ argument optionally
  provided by the user (see items 1 and 2 above). Below is an example of
  such files:
   #+begin_example
   Username,Last Name,First Name,Lab 0X Points Grade <Course Data>,End-of-Line-Indicator
   #TTiger,Tigertongue,Tim,100.0,# 
   #Patrick97,Pearson,Patrick,72.5,#
   #Towhander,Twohands,Tony,100.0,#
   #Zain1997,Zodson,Zain,95.5,#
   #Coopercat,Cooper,Cain,100.0,#
   #Hammermann,Odinson,Thor,0.0,#
   #+end_example
   Note:
   - If a student exists in the exported file but not in the
     submissions folder, then the respective grades will
     not be included in the generated in the respective csv files.
- A Feedback folder that holds feedback files for the students. The
  general structure is like this: Consider Timb Handerson who did not
  get a full grade. His feedback file will be as such:
  #+begin_example
  Feedback on your assignment solution

  Unit test results:
  
  ((Pass TEST-DEPOSIT (EQUAL (DEPOSIT 20) 130))
   (Pass TEST-DEPOSIT (EQUAL (DEPOSIT 10) 110))
   (Pass TEST-DEPOSIT (NOT (DEPOSIT 10001)))
   (Fail TEST-WITHDRAW (EQUAL (WITHDRAW 60) 10))
   (Pass TEST-WITHDRAW (NOT (WITHDRAW 80)))
   (Pass TEST-WITHDRAW (NOT (WITHDRAW 10001)))
   (Fail TEST-WITHDRAW (EQUAL (WITHDRAW 20) 70))
   (Fail TEST-WITHDRAW (EQUAL (WITHDRAW 10) 90)))
  #+end_example

The log file *codegrader-history/log.txt* located in the root of the
user's home directory contains historical information about the
evaluation of students' assignments.

* Other functions

** Marking all the program files submitted by a student

In case you wish to mark all the program files submitted by a student, you can use the following function:
#+begin_example
eval-student-solutions (std-id solutions-folder test-cases-folder output-folder)
evaluate-solution (student-solution test-cases-dir)
---------------------------------------------------
Description:  Based on the given student id (std-id, an integer), the students' solutions in solutions-folder, and 
              the test cases in test-cases-folder, generates a file in the output-folder containing the CodeGrader-generated feedback.
Inputs:       1) std-id [integer]: The student id number
              2) solutions-folder [string]: the full path of the folder containing the student's program files
              3) test-cases-folder [string]: The folder containing the test cases files.
              4) output-folder [string]: An existing folder where the generated feedback file will be saved

Outputs:      
              [string] A message informing where the feedback file has been saved.
---------------------------------------------------
#+end_example

Usage Example: John is a student whose ID is 1234. Say you needed to
autograde John's solutions stored in =/home/John/Solutions/=. The test
cases are stored in =/home/John/Test-cases/=, and you want to store the feedback in =/home/John/Results/=
#+begin_src lisp
  CL-USER> (ql:quickload :codegrader)  ; Loading the codegrader
  CL-USER> (cg:eval-student-solutions "/home/John/Solutions/" "/home/John/Test-cases/" "/home/John/Results/")
  Feedback saved in /home/John/Results/3753443020201070578.txt
  CL-USER> 
#+end_src


** Marking one program file submitted by a student
In case you wish to mark one specific submission or test your test
case file, you can use the following function:
#+begin_example
evaluate-solution (student-solution test-cases-dir)
---------------------------------------------------
Description:  Loads the student-solution file, loads the test cases, runs
              the test cases, and returns the percentage of correct results over total results

Inputs:       1) student-solution [string]: The directory for the solution of the student.
              2) test-cases-dir [string]: The directory for the test cases file. This will be used to test the solution of the students for the current assignment.

Outputs:      [list] A list of the following:
              1) [string] The grade of the student.
              2) [string] A comment that describes if there was a runtime error while loading the student submission or not
              3) [string] A description of what happened during runtime (from exceptions to conditions to whatever) 
              4) [list] The results of marking each of the test cases.

Side-effects: This function utilizes the global variable *results* while running. In the beginning by reseting it to nil, and at the end by updating it with the current
              student's submission results.
---------------------------------------------------
#+end_example

Usage Example: Say there was a student that you want to mark their
submissions independantly from the other students. You can simply take
their lisp submission file, say ~"/home/John/mysol.lisp"~ , and the
test cases lisp file "/home/john/test-cases.lisp"~. You would use 
CodeGrader as follows: (assuming you have already installed CodeGrader
as shown above)
#+begin_src lisp
  CL-USER> (ql:quickload :codegrader)  ; Loading the codegrader
  CL-USER> (cg:evaluate-solution "/home/John/mysol.lisp" "/home/John/test-cases.lisp") 
  ("100.0" OK "No runtime errors"
   (("Pass" T TEST-DEPOSIT (EQUAL (DEPOSIT 20) 130))
    ("Pass" T TEST-DEPOSIT (EQUAL (DEPOSIT 10) 110))
    ("Pass" T TEST-DEPOSIT (NOT (DEPOSIT 10001)))
    ("Pass" T TEST-WITHDRAW (EQUAL (WITHDRAW 60) 10))
    ("Pass" T TEST-WITHDRAW (NOT (WITHDRAW 80)))
    ("Pass" T TEST-WITHDRAW (NOT (WITHDRAW 10001)))
    ("Pass" T TEST-WITHDRAW (EQUAL (WITHDRAW 20) 70))
    ("Pass" T TEST-WITHDRAW (EQUAL (WITHDRAW 10) 90))))
  GRADER> (in-package :CL-USER)
  CL-USER> 
#+end_src

  
* License and Credits

See LICENSE for usage permissions. See AUTHORS for credits.




