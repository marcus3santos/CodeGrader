#+TITLE: CodeGrader
#+Options: tc:t

CodeGrader is an automated grader for Lisp programming practicum
assessments.


* Dependencies

Follow the instructions available [[https://marcus3santos.github.io/lisp-ide.html][here to install the Lisp programming environment]].

  
* Installation

Type the commands below an a linux or Mac terminal:
  #+begin_src shell
   cd ~/quicklisp/local-projects/
   git clone https://github.com/marcus3santos/codegrader.git  
  #+end_src

* The Workflow

The workflow involves the following steps:

1. The instructor writes the assessment in s-expression markup (SXM).
2. The instructor uses CodeGrader to generate from the SXM file the necessary support
   files, including the formatted questions translated to orgmode and assessment-specific test
   cases to be later used by CodeGrader to grade the students' solutions.
3. Students complete the assessment created by the instructor,
   providing their solutions to the given programming questions.
4. The instructor uses CodeGrader to evaluate and grade the students'
   solutions.
The next sections explain each of these workflow steps.

* Writing a Lisp Programming Assessment Using SXM

Instructors write their Lisp assessments using the S-expresbsion Markup
format (SXM). This structured format simplifies parsing and enables
better automation and tooling.

Each SXM file should be a valid Lisp-style expression.

** SXM Structure Overview

#+begin_example
(doc ;; The assessment description
  (:title "Common Lisp Practicum Test"
   :folder "~/pt") ;; Where students are required to store their solutions
  (q ;; a question description; transated as a level 1 section "*" in orgmode
    (:title "Question"
     :number 1 
     :penalty 80 ;;  the deduction in the student's score if they use a forbidden function
     :forbidden (count member)) ;; the list of forbidden functions
    (p "This question contains two parts.") ;; a paragraph.
    (wa  ;; What students are asked; must appear within a question expression
      (p "Part 1:)
      (p "Write a function COUNT-OCCURRENCES that takes an element and a list as arguments and returns the number of times the element appears in the list.")
        (tc (:function count-occurrences)
         (gvn ;; Given test cases           
          (a ;; an assertion specifying a function call and its expected result
            (count-occurrences 3 '(1 2 3 3 3 4)) 3)
          (a (count-occurrences 'a '(a b a c a)) 3)
          (a (count-occurrences 5 '(1 2 3 4)) 0))
         (hdn ;; Hidden test cases used for grading a student's solution
          (a (count-occurrences 1 '(1 1 1 1 1)) 5)
          (a (count-occurrences 0 '(1 2 3 4)) 0)
          (a (count-occurrences 'z '(a b c z z)) 2))
      (p "Part 2:")
      (p "...")
        ;; Additional example/test blocks can go here
       ))
  (q ;; other question...))
#+end_example

** Explanation of Fields

- =:title= — Assessment title.
- =:folder= — Directory path where students are required to save their solutions.
- =q= — A section describing an individual exercise or problem.
  - =:title= — The question title, e.g.  `"Question"` or `"Exercise"`.
  - =:number= — Integer identifier for the question.
  - =:penalty= — (Optional) Penalty percentage for using forbidden functions.
  - =:forbidden= — (Optional) List of forbidden functions in Lisp syntax.
  - =p= — a paragraph of text.
  - =wa= — A section describing what the student is being asked. It can contain:
    - =(p ...)= - paragraphs
    - =(tc ...)= - test cases
    - =(gvn ...)=- Node  containing given test assertions; they are translated to code that students can use to test their solutions during the test.
    - =(hdn ...)= - Node containing hidden test assertions; they are translated to code that CodeGrader will use to test the students' solutions after they complete the test.

* Generating the Assessment Support Files

Assuming the SXM description for your assessment is located in the file `~/tmp/assessment1.sxm`.

#+begin_src lisp
  CL-USER> (in-package :SXM)
  ...
  SXM> (gen-exam-files "~/tmp/assessment1.sxm")
  Generated assessment orgmode description file at: ~/tmp/assessment1.org
  Generated assessment testing code at: ~/tmp/assessment1.data
  SXM>
#+end_src

CodeGrader's `GEN-EXAM-FILES` function generates the following files within the `./Gen-files/` directory:

- An Org mode file (=~/tmp/assessment1.org=) containing the assessment description. This file can be further processed using Emacs to export it to various formats such as HTML or PDF.
- A tooling data file (=~/tmp/assessment1.data=) that CodeGrader utilizes for the automated grading of student solutions.


* Student Interaction with the Assessment  

As it is typically the case, during a programming practicum assessment students are tasked with
solving programming questions provided in the assessment description. This stage
involves students working through the programming problems, adhering
to the specified requirements, and crafting solutions that meet the
criteria outlined in the assessment.

The rewritten assessment specifies two key requirements:  
1. *Allowed Lisp Functions*: Students must avoid using any forbidden
   functions listed for each question. CodeGrader applies penalties
   for violations. As seen earlier, the instructor defines this
   penalties in the assessment description.
2. *File Location and Naming*: Students must save their program files
   in the designated folder and with the specified names, as outlined
   in the header of the assessment. As seen earlier, the instructor
   also defines the file location in the assessment description.

To assist students in adhering to these requirements and verifying
their progress, CodeGrader provides the function =CHK-MY-SOLUTION=. Students can use this function during the assessment to:
- *Check Compliance*: Ensure their solution file is saved in the
  correct folder and follows the naming convention specified in the
  assessment.
- *Run Example Test Cases*: Execute their solution against the
  examples included in the assessment description to verify
  correctness for those specific cases.

*For example*: Suppose a student wants to verify whether their solution to Question 1 of Practicum Test 1 meets the assessment requirements. Assuming that solutions must be saved in the *~/pt1/* folder, the student can evaluate the following expression in the Lisp REPL to check their work after completing their solution: 
#+begin_example  
CL-USER> (CHK-MY-SOLUTION "~/pt1/q1.lisp")  
#+end_example  

This function will:  
1. Validate that the solution is stored in the correct folder with the required name.  
2. Execute the example test cases for *Question 1* and provide feedback on whether the solution passes these tests.  

By using =CHK-MY-SOLUTION=, students can identify and address
potential issues early, ensuring their submissions align with the assessment's specifications.

* Grading students' solutions

** Preamble

We make the following assumptions regarding the physical environment where students complete the assessment:
- The IT staff has created a spreadsheet mapping each student ID to a computer ID in the exam room.
- That mapping is shared with the instructor and the students in advance of the assessment.
- In the Linux test environment, the home folder's name is the computer ID.
- In the assessment description, students were asked to store their solutions in the *~/pt/* folder.
- After students completed the assessment, the IT staff has transfered the contents of the *~/pt/* folders of all machines to the */tmp/cps305PracticumTest/cps305XX* folder, where *cps305XX* represents different folders (e.g., cps30501, cps30502, etc.) storing students' solutions from different assesssment versions.
- The IT staff has created a zip archive file containing the students' solutions originally stored in specific the *~/pt/~ folders of the Linux machines. Here is the command the IT staff used to create that zip file:
  #+begin_src shell
  zip -r cps305PracticumTest.zip /tmp/cps305PracticumTest/cps305*/<computer ID>*/ -x '*/.*' '*/quicklisp/*' '*/Cheatsheet-emacs.pdf' '*/paredit.pdf' '*/test.pdf'
  #+end_src
  

** Steps for grading students' solutions

1. *Create a zipped file containing the students' solutions*: Since
   students from specific sections may have taken different versions
   of the exam, it is crucial to obtain the sections-to-exam-versions
   mapping in advance from the course coordinator to ensure you are running
   CodeGrader  on the solutions written by the students in the correct section, and
   using the correct test cases for the respective exam version.  For
   example, suppose students from sections 03 and 05 are taught by instructor A and took Version 1 of
   the exam, and students from section 10 are taught by instructor B and took Version 2. 

   Assume the parent folder *~/tmp/cps305PracticumTest/* contains the students' solutions from the various sections. Also assume you want to store the zip archive for sections 03 and 05
   in *~/tmp/PT1/Sections/03-05/* (you have already created that folder), the commands below show how to
   create the zip file with the solutions of the students from sectoins 03 and 05:
   #+begin_src shell
     cd ~/tmp/cps305PracticumTest
     (cd cps30503 && zip -r ~/tmp/PT1/Sections/03-05/std-sol.zip *) && (cd cps30505 && zip -r ~/tmp/PT1/Sections/03-05/std-sol.zip *)
   #+end_src
   You would do something similar to zip the solutions for students in section 10.
   #+begin_src shell
     cd ~/tmp/cps305PracticumTest/cps30510
     zip -r ~/tmp/PT1/Sections/10/std-sol.zip 
   #+end_src
   By zipping this way, you would create a zip archive that does not
   include the parent directories (cps30505 and cps30508).

2. *Create a CSV file containing the mapping of students-to-computers*:
   We assume the IT technicians have sent you CSV files containing the
   student-to-computer mapping for each of the course sections. Now,
   based on these CSV files and on the sections-to-exams-versions, you
   should create a CSV file that contains the mappings of all students
   who took a given test version. Each row in that
   spreadsheet should contain the following information: Student ID
   number, Student First Name, Student Last Name, and Room-PC ID
3. *Generate the tooling (.data) file* as explained in section *Generating the Assessment Support Files* above.
4. Create a folder where CodeGrader will store the results. You can give any name to that folder.
5. [ /This is step is not necessary if you are assessing an "ungraded" examination (i.e., an assessment
   whose weight is zero)/ ] On D2L, export the students' *assignment grades* *to a CSV
   file*. Note the following when generating this file
   - Select the following /Export Options/:
     - Key Field:
       - *Both*
     - Grade Values:
       - *Points grade*
     - User Details: 
       - *Last name*
       - *First name*
  - /Choose grades to Export/: Choose only one of the listed grade items. If the grade item contains subitems, choose the appropriate subitem. For example: if a /Practicum Test/ grade item contains subitems representing the versions of the test, choose the subitem representing the Practicum Test version you are interested in grading.
  Below is an example of a CSV file exported by D2L:
  #+begin_example
     OrgDefinedId,Username,Last Name,First Name,Practicum Test 1 - Version 1 Points Grade <Numeric MaxPoints:100 Weight:10 Category:Practicum Test 1 CategoryWeight:10>,End-of-Line Indicator
     #500583619,#TTiger,Tigertongue,Tim,,#
     #500585612,#Patrick97,Pearson,Patrick,,#
     #501585619,#Towhander,Twohands,Tony,,#
     #500586619,#Zain1997,Zodson,Zain,,#
     #500585619,#Coopercat,Cooper,Cain,,#
     #500585119,#Hammermann,Odinson,Thor,,#
  #+end_example
  For more information, visit [[https://www.torontomu.ca/courses/instructors/tutorials/grades/grades-export-import/]]
6. Launch sbcl from the command line
   #+begin_src shell
     rlwrap sbcl --dynamic-space-size 20480
   #+end_src
7. To load the codegrader, type the following commands on the REPL:
   #+begin_src lisp
          (ql:quickload :codegrader)
   #+end_src
8. To run the students' solutions through CodeGrader, type the command
   below on the CodeGrader REPL: (NOTE: once you launch CodeGrader, it
   will start executing the students' solutions; consequently, it will
   display on the REPL buffer all error/warning messages and output
   generated by the student's solution. CodeGrader will be done
   marking when you see the message =Exam grading complete!= displayed
   on the REPL window buffer.)
   #+begin_src lisp
     (cg:grade-exam submissions map tooling-file results-folder exam-grades-export-file)
   #+end_src
   where:
   - ~submissions~ is a string representing the full path and name of
     the zipped file containing the students' solutions, e.g.,
     ~/Users/johndoe/Zipped-solutions/std-sol.zip~
   - ~map~  is a string representing the full path and name of of the csv file storing the student-to-pc mapping.
   - ~tooling-file~ is a string representing the full path for the tooling (.data) file.
   - ~results-folder~ is a string representing the full path for a folder
    where you want codegrader to store the results (the students'
    marks and log files). For example, if you provide the path
    #+begin_example
    "/Users/johndoe/A1/"
    #+end_example
    then CodeGrader will create its
    files/subfolders inside folder ~/Users/johndoe/A1/~.
   - (optional) ~exam-grades-export-file~ is a string representing the full path for
     the D2L exam grades exported by D2L

* Output

CodeGrader generates the following files in the =results= folder (see above):
- A csv spreadsheet file called ~grades.csv~  This is a D2L-importable
  grades file and it is created based on the ~exam-grades-export-file~ argument optionally
  provided by the user (see items 1 and 2 above). Below is an example of
  such files:
   #+begin_example
   Username,Last Name,First Name,Lab 0X Points Grade <Course Data>,End-of-Line-Indicator
   #TTiger,Tigertongue,Tim,100.0,# 
   #Patrick97,Pearson,Patrick,72.5,#
   #Towhander,Twohands,Tony,100.0,#
   #Zain1997,Zodson,Zain,95.5,#
   #Coopercat,Cooper,Cain,100.0,#
   #Hammermann,Odinson,Thor,0.0,#
   #+end_example
   Note:
   - If a student exists in the exported file but not in the
     submissions folder, then the respective grades will
     not be included in the generated in the respective csv files.
- A Feedback folder that holds feedback files for the students. The
  general structure is like this: Consider Timb Handerson who did not
  get a full grade. His feedback file will be as such:
  #+begin_example
  Feedback on your assignment solution

  Unit test results:
  
  ((Pass TEST-DEPOSIT (EQUAL (DEPOSIT 20) 130))
   (Pass TEST-DEPOSIT (EQUAL (DEPOSIT 10) 110))
   (Pass TEST-DEPOSIT (NOT (DEPOSIT 10001)))
   (Fail TEST-WITHDRAW (EQUAL (WITHDRAW 60) 10))
   (Pass TEST-WITHDRAW (NOT (WITHDRAW 80)))
   (Pass TEST-WITHDRAW (NOT (WITHDRAW 10001)))
   (Fail TEST-WITHDRAW (EQUAL (WITHDRAW 20) 70))
   (Fail TEST-WITHDRAW (EQUAL (WITHDRAW 10) 90)))
  #+end_example

The log file *codegrader-history/log.txt* located in the root of the
user's home directory contains historical information about the
evaluation of students' assignments.

* Other functions

** Marking all the program files submitted by a student

In case you wish to mark all the program files submitted by a student, you can use the following function:
#+begin_example
eval-student-solutions (std-id solutions-folder test-cases-folder output-folder)
evaluate-solution (student-solution test-cases-dir)
---------------------------------------------------
Description:  Based on the given student id (std-id, an integer), the students' solutions in solutions-folder, and 
              the test cases in test-cases-folder, generates a file in the output-folder containing the CodeGrader-generated feedback.
Inputs:       1) std-id [integer]: The student id number
              2) solutions-folder [string]: the full path of the folder containing the student's program files
              3) test-cases-folder [string]: The folder containing the test cases files.
              4) output-folder [string]: An existing folder where the generated feedback file will be saved

Outputs:      
              [string] A message informing where the feedback file has been saved.
---------------------------------------------------
#+end_example

Usage Example: John is a student whose ID is 1234. Say you needed to
autograde John's solutions stored in =/home/John/Solutions/=. The test
cases are stored in =/home/John/Test-cases/=, and you want to store the feedback in =/home/John/Results/=
#+begin_src lisp
  CL-USER> (ql:quickload :codegrader)  ; Loading the codegrader
  CL-USER> (cg:eval-student-solutions "/home/John/Solutions/" "/home/John/Test-cases/" "/home/John/Results/")
  Feedback saved in /home/John/Results/3753443020201070578.txt
  CL-USER> 
#+end_src


** Marking one program file submitted by a student
In case you wish to mark one specific submission or test your test
case file, you can use the following function:
#+begin_example
evaluate-solution (student-solution test-cases-dir)
---------------------------------------------------
Description:  Loads the student-solution file, loads the test cases, runs
              the test cases, and returns the percentage of correct results over total results

Inputs:       1) student-solution [string]: The directory for the solution of the student.
              2) test-cases-dir [string]: The directory for the test cases file. This will be used to test the solution of the students for the current assignment.

Outputs:      [list] A list of the following:
              1) [string] The grade of the student.
              2) [string] A comment that describes if there was a runtime error while loading the student submission or not
              3) [string] A description of what happened during runtime (from exceptions to conditions to whatever) 
              4) [list] The results of marking each of the test cases.

Side-effects: This function utilizes the global variable *results* while running. In the beginning by reseting it to nil, and at the end by updating it with the current
              student's submission results.
---------------------------------------------------
#+end_example

Usage Example: Say there was a student that you want to mark their
submissions independantly from the other students. You can simply take
their lisp submission file, say ~"/home/John/mysol.lisp"~ , and the
test cases lisp file "/home/john/test-cases.lisp"~. You would use 
CodeGrader as follows: (assuming you have already installed CodeGrader
as shown above)
#+begin_src lisp
  CL-USER> (ql:quickload :codegrader)  ; Loading the codegrader
  CL-USER> (cg:evaluate-solution "/home/John/mysol.lisp" "/home/John/test-cases.lisp") 
  ("100.0" OK "No runtime errors"
   (("Pass" T TEST-DEPOSIT (EQUAL (DEPOSIT 20) 130))
    ("Pass" T TEST-DEPOSIT (EQUAL (DEPOSIT 10) 110))
    ("Pass" T TEST-DEPOSIT (NOT (DEPOSIT 10001)))
    ("Pass" T TEST-WITHDRAW (EQUAL (WITHDRAW 60) 10))
    ("Pass" T TEST-WITHDRAW (NOT (WITHDRAW 80)))
    ("Pass" T TEST-WITHDRAW (NOT (WITHDRAW 10001)))
    ("Pass" T TEST-WITHDRAW (EQUAL (WITHDRAW 20) 70))
    ("Pass" T TEST-WITHDRAW (EQUAL (WITHDRAW 10) 90))))
  GRADER> (in-package :CL-USER)
  CL-USER> 
#+end_src

* Sandboxing - Package Structure for Exporting Functions to test-runtime

The security mechanism that isolates the student's code so preventing its execution environment to
prevent from affecting the CodeGrader is defined in the =sandbox= package. 

Depending on the content of the assessment description file, function
GEN-EXAM-FILES modifies the =sandbox= package based on the information
provided in the assessment description file.

This sandboxing mechanism is structured as follows:
1. =sandbox= package:
   - Restricts system-level access when running the student's code.
   - Exports only specific functions to be accessed by the =test-runtime= package,
     which executes those functions against predefined test cases.
2. =test-runtime= package:
   - Utilizes the =sandbox= package to access its exported functions.
   - Exports objects and functions for use by other packages that
     manage additional aspects of grading.

** Benefits
- **Encapsulation**: Only necessary functions are exported, hiding internal details.
- **Reusability**: `=test-runtime= focuses on testing with a consistent interface.
- **Flexibility**: Function GEN-EXAM-FILEs facilitates the
  reconfiguration of the sandbox mechanism depending on the
  characteristics of the assessment description.
  
* License and Credits

See LICENSE for usage permissions. See AUTHORS for credits.




